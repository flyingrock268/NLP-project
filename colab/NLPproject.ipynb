{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Colab defaults to Python 3.9 when we need python 3.7\n",
        "This section downloads the right version and changes it to it"
      ],
      "metadata": {
        "id": "b_XXixRbOIgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.7\n",
        "from IPython.display import clear_output \n",
        "clear_output()\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "\n",
        "# Choose one of the given alternatives:\n",
        "!sudo update-alternatives --config python3\n",
        "\n",
        "# This one used to work but now NOT(for me)!\n",
        "# !sudo update-alternatives --config python\n",
        "\n",
        "# Check the result\n",
        "!python3 --version\n",
        "\n",
        "# Attention: Install pip (... needed!)\n",
        "!sudo apt install python3-pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VA_YGy8r_pV",
        "outputId": "5cf3b1f7-b2db-4af1-edb3-44e9e55db233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.9   2         auto mode\n",
            "  1            /usr/bin/python3.7   1         manual mode\n",
            "  2            /usr/bin/python3.8   1         manual mode\n",
            "  3            /usr/bin/python3.9   2         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 1\n",
            "update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in manual mode\n",
            "Python 3.7.16\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-525\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 28 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n",
            "Fetched 2,389 kB in 0s (5,316 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 128913 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section gets pip working"
      ],
      "metadata": {
        "id": "P-SMIAbGOVfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python3.7-distutils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFCbA6mZsB3j",
        "outputId": "99c57099-e77a-4394-f7bc-525e5e077703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-525\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  python3.7-lib2to3\n",
            "The following NEW packages will be installed:\n",
            "  python3.7-distutils python3.7-lib2to3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 28 not upgraded.\n",
            "Need to get 309 kB of archives.\n",
            "After this operation, 1,229 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.7-lib2to3 all 3.7.16-1+focal1 [122 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.7-distutils all 3.7.16-1+focal1 [187 kB]\n",
            "Fetched 309 kB in 1s (252 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3.7-lib2to3.\n",
            "(Reading database ... 129277 files and directories currently installed.)\n",
            "Preparing to unpack .../python3.7-lib2to3_3.7.16-1+focal1_all.deb ...\n",
            "Unpacking python3.7-lib2to3 (3.7.16-1+focal1) ...\n",
            "Selecting previously unselected package python3.7-distutils.\n",
            "Preparing to unpack .../python3.7-distutils_3.7.16-1+focal1_all.deb ...\n",
            "Unpacking python3.7-distutils (3.7.16-1+focal1) ...\n",
            "Setting up python3.7-lib2to3 (3.7.16-1+focal1) ...\n",
            "Setting up python3.7-distutils (3.7.16-1+focal1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check to make sure that it's the right version of python 3.7.16"
      ],
      "metadata": {
        "id": "axYhyKUbOZWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIIFTHSqsho5",
        "outputId": "6d028796-cbf1-43c9-8489-25682d4fd33c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unzip data after you upload"
      ],
      "metadata": {
        "id": "miWiGTjBOego"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "le7Pj9rixtyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "installs requirements, this may take a while"
      ],
      "metadata": {
        "id": "jOaHcQokOkA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Biu-NRqWtBrZ",
        "outputId": "247f30f6-3c06-44b3-a020-e2cf1e035567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting nltk==3.7\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 60.1 MB/s \n",
            "\u001b[?25hCollecting torch==1.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 400 bytes/s \n",
            "\u001b[?25hCollecting torchaudio==0.10.0\n",
            "  Downloading https://download.pytorch.org/whl/rocm4.1/torchaudio-0.10.0%2Brocm4.1-cp37-cp37m-linux_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 36.2 MB/s \n",
            "\u001b[?25hCollecting transformers==4.13\n",
            "  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 75.3 MB/s \n",
            "\u001b[?25hCollecting accelerate==0.5.1\n",
            "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting datasets==2.4.0\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 67.2 MB/s \n",
            "\u001b[?25hCollecting spacy==3.4.2\n",
            "  Downloading spacy-3.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 56.6 MB/s \n",
            "\u001b[?25hCollecting typing-extensions==4.2.0\n",
            "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
            "Collecting pydantic==1.10.2\n",
            "  Downloading pydantic-1.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 52.4 MB/s \n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[K     |████████████████████████████████| 757 kB 71.3 MB/s \n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 77.3 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.10.7-py3-none-any.whl (10 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 77.1 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading importlib_metadata-6.1.0-py3-none-any.whl (21 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 68.6 MB/s \n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 72.1 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 70.2 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
            "\u001b[K     |████████████████████████████████| 143 kB 79.3 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting pyarrow>=6.0.0\n",
            "  Downloading pyarrow-11.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 35.1 MB 300 kB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.10\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 72.7 MB/s \n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[K     |████████████████████████████████| 490 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 67.6 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy==3.4.2->-r requirements.txt (line 9)) (45.2.0)\n",
            "Collecting wasabi<1.1.0,>=0.9.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting thinc<8.2.0,>=8.1.0\n",
            "  Downloading thinc-8.1.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (912 kB)\n",
            "\u001b[K     |████████████████████████████████| 912 kB 74.2 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 64.5 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 101 kB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 57.3 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\n",
            "\u001b[K     |████████████████████████████████| 171 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pytz>=2017.3\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[K     |████████████████████████████████| 502 kB 74.7 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.7.3\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 76.2 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting attrs>=17.3.0\n",
            "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 77.7 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0; python_version < \"3.8\"\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 40.2 MB/s \n",
            "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895255 sha256=e5f9dd07dcd40a9f44a71c547f251f454a195418776d4d0a522e897c73dc2a46\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: multiprocess 0.70.14 has requirement dill>=0.3.6, but you'll have dill 0.3.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 3.4.2 has requirement typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you'll have typing-extensions 4.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: regex, typing-extensions, zipp, importlib-metadata, click, joblib, tqdm, nltk, numpy, torch, torchaudio, certifi, idna, urllib3, charset-normalizer, requests, filelock, packaging, pyyaml, huggingface-hub, tokenizers, six, sacremoses, transformers, accelerate, dill, pytz, python-dateutil, pandas, multiprocess, frozenlist, aiosignal, async-timeout, multidict, attrs, yarl, asynctest, aiohttp, fsspec, responses, pyarrow, xxhash, datasets, pydantic, murmurhash, cymem, preshed, spacy-legacy, MarkupSafe, jinja2, spacy-loggers, catalogue, srsly, langcodes, typer, wasabi, blis, confection, thinc, smart-open, pathy, spacy\n",
            "Successfully installed MarkupSafe-2.1.2 accelerate-0.5.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 asynctest-0.13.0 attrs-22.2.0 blis-0.7.9 catalogue-2.0.8 certifi-2022.12.7 charset-normalizer-3.1.0 click-8.1.3 confection-0.0.4 cymem-2.0.7 datasets-2.4.0 dill-0.3.5.1 filelock-3.10.7 frozenlist-1.3.3 fsspec-2023.1.0 huggingface-hub-0.13.3 idna-3.4 importlib-metadata-6.1.0 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 multidict-6.0.4 multiprocess-0.70.14 murmurhash-1.0.9 nltk-3.7 numpy-1.21.5 packaging-23.0 pandas-1.3.5 pathy-0.10.1 preshed-3.0.8 pyarrow-11.0.0 pydantic-1.10.2 python-dateutil-2.8.2 pytz-2023.3 pyyaml-6.0 regex-2022.10.31 requests-2.28.2 responses-0.18.0 sacremoses-0.0.53 six-1.16.0 smart-open-6.3.0 spacy-3.4.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 tokenizers-0.10.3 torch-1.10.0+cu111 torchaudio-0.10.0+rocm4.1 tqdm-4.65.0 transformers-4.13.0 typer-0.4.2 typing-extensions-4.2.0 urllib3-1.26.15 wasabi-0.10.1 xxhash-3.2.0 yarl-1.8.2 zipp-3.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "dateutil",
                  "six",
                  "zipp"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download spacy"
      ],
      "metadata": {
        "id": "mSO1O5dPOp0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "R0WJTa14uYHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1608b912-fb47-414a-c0cb-40b8c05d9479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 598 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (45.2.0)\n",
            "Collecting typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\"\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.65.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.1.0)\n",
            "Installing collected packages: en-core-web-sm, typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "Successfully installed en-core-web-sm-3.4.1 typing-extensions-4.1.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the data preperation, this WILL take a while"
      ],
      "metadata": {
        "id": "5XrtiIk9O5zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash data_preparation.sh"
      ],
      "metadata": {
        "id": "ZizM8bAFwfY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f224f872-07fe-4fda-e530-8a3870b05bf0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= = = = = = = = = = = = = =\n",
            "The project is running...\n",
            "Wed 05 Apr 2023 05:57:12 AM UTC\n",
            "= = = = = = = = = = = = = =\n",
            "perform Speaker Swap * 5 times\n",
            "100% 6/6 [00:04<00:00,  1.41it/s]\n",
            "100% 6/6 [00:04<00:00,  1.42it/s]\n",
            "100% 6/6 [00:04<00:00,  1.36it/s]\n",
            "100% 6/6 [00:03<00:00,  1.84it/s]\n",
            "100% 6/6 [00:02<00:00,  2.05it/s]\n",
            "perform Entity Swap * 5 times\n",
            "100% 6/6 [00:03<00:00,  1.95it/s]\n",
            "100% 6/6 [00:04<00:00,  1.43it/s]\n",
            "100% 6/6 [00:04<00:00,  1.32it/s]\n",
            "100% 6/6 [00:04<00:00,  1.34it/s]\n",
            "100% 6/6 [00:03<00:00,  1.80it/s]\n",
            "perform Date Swap * 5 times\n",
            "100% 6/6 [00:03<00:00,  1.96it/s]\n",
            "100% 6/6 [00:03<00:00,  1.95it/s]\n",
            "100% 6/6 [00:04<00:00,  1.31it/s]\n",
            "100% 6/6 [00:04<00:00,  1.34it/s]\n",
            "100% 6/6 [00:04<00:00,  1.35it/s]\n",
            "perform Number Swap * 5 times\n",
            "100% 6/6 [00:03<00:00,  1.89it/s]\n",
            "100% 6/6 [00:03<00:00,  1.99it/s]\n",
            "100% 6/6 [00:03<00:00,  1.99it/s]\n",
            "100% 6/6 [00:04<00:00,  1.33it/s]\n",
            "100% 6/6 [00:04<00:00,  1.35it/s]\n",
            "perform Pronoun Swap * 5 times\n",
            "100% 6/6 [00:07<00:00,  1.20s/it]\n",
            "100% 6/6 [00:05<00:00,  1.08it/s]\n",
            "100% 6/6 [00:07<00:00,  1.27s/it]\n",
            "100% 6/6 [00:08<00:00,  1.36s/it]\n",
            "100% 6/6 [00:05<00:00,  1.09it/s]\n",
            "perform Negation * 5 times\n",
            "100% 6/6 [00:03<00:00,  1.90it/s]\n",
            "100% 6/6 [00:04<00:00,  1.29it/s]\n",
            "100% 6/6 [00:04<00:00,  1.30it/s]\n",
            "100% 6/6 [00:04<00:00,  1.31it/s]\n",
            "100% 6/6 [00:03<00:00,  1.95it/s]\n",
            "= = = = = = = = = = = = = =\n",
            "The project is Finished...\n",
            "The program takes '2' minutes.\n",
            "= = = = = = = = = = = = = =\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unzip trained model before running the eval"
      ],
      "metadata": {
        "id": "DFngyH5qOuH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip trained_model.zip"
      ],
      "metadata": {
        "id": "wgrE2qYeMdbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_demo.sh"
      ],
      "metadata": {
        "id": "z5R9EZREqR7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2584c400-bcca-43a1-ed42-01eddb1c4fdb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= = = = = = = = = = = = = =\n",
            "The project is running...\n",
            "Wed 05 Apr 2023 04:58:18 AM UTC\n",
            "= = = = = = = = = = = = = =\n",
            "04/05/2023 04:58:19 - INFO - root - *** Parameters ***\n",
            "04/05/2023 04:58:19 - INFO - root - load_file: ./data/faceval_samples.json\n",
            "04/05/2023 04:58:19 - INFO - root - neg_marker: _neg\n",
            "04/05/2023 04:58:19 - INFO - root - ignore_pad_token_for_loss: True\n",
            "04/05/2023 04:58:19 - INFO - root - max_source_length: 1024\n",
            "04/05/2023 04:58:19 - INFO - root - source_prefix: None\n",
            "04/05/2023 04:58:19 - INFO - root - preprocessing_num_workers: None\n",
            "04/05/2023 04:58:19 - INFO - root - overwrite_cache: True\n",
            "04/05/2023 04:58:19 - INFO - root - min_target_length: 1\n",
            "04/05/2023 04:58:19 - INFO - root - max_target_length: 128\n",
            "04/05/2023 04:58:19 - INFO - root - length_penalty: 1.0\n",
            "04/05/2023 04:58:19 - INFO - root - num_beams: None\n",
            "04/05/2023 04:58:19 - INFO - root - pad_to_max_length: False\n",
            "04/05/2023 04:58:19 - INFO - root - model_name_or_path: ./trained_model/\n",
            "04/05/2023 04:58:19 - INFO - root - config_name: None\n",
            "04/05/2023 04:58:19 - INFO - root - tokenizer_name: None\n",
            "04/05/2023 04:58:19 - INFO - root - text_column: dialogue\n",
            "04/05/2023 04:58:19 - INFO - root - summary_column: summary\n",
            "04/05/2023 04:58:19 - INFO - root - use_slow_tokenizer: False\n",
            "04/05/2023 04:58:19 - INFO - root - per_device_train_batch_size: 8\n",
            "04/05/2023 04:58:19 - INFO - root - per_device_eval_batch_size: 8\n",
            "04/05/2023 04:58:19 - INFO - root - per_device_test_batch_size: 8\n",
            "04/05/2023 04:58:19 - INFO - root - learning_rate: 5e-05\n",
            "04/05/2023 04:58:19 - INFO - root - weight_decay: 0.0\n",
            "04/05/2023 04:58:19 - INFO - root - num_train_epochs: 3\n",
            "04/05/2023 04:58:19 - INFO - root - max_train_steps: None\n",
            "04/05/2023 04:58:19 - INFO - root - gradient_accumulation_steps: 1\n",
            "04/05/2023 04:58:19 - INFO - root - lr_scheduler_type: SchedulerType.LINEAR\n",
            "04/05/2023 04:58:19 - INFO - root - num_warmup_steps: 0\n",
            "04/05/2023 04:58:19 - INFO - root - output_dir: ./scores_log/\n",
            "04/05/2023 04:58:19 - INFO - root - cache_dir: ./output/cache\n",
            "04/05/2023 04:58:19 - INFO - root - seed: 12345\n",
            "04/05/2023 04:58:19 - INFO - root - model_type: bart\n",
            "04/05/2023 04:58:19 - INFO - root - shuffle: False\n",
            "04/05/2023 04:58:19 - INFO - root - label_smoothing: 0.0\n",
            "04/05/2023 04:58:19 - INFO - root - tal: 0.3\n",
            "04/05/2023 04:58:19 - INFO - root - alpha: 1.0\n",
            "04/05/2023 04:58:19 - INFO - root - train_ratio: 1.0\n",
            "04/05/2023 04:58:19 - INFO - root - corrupt_ratio: 0.0\n",
            "04/05/2023 04:58:19 - INFO - root - cf_pair: False\n",
            "04/05/2023 04:58:19 - INFO - root - cf_augmentation: False\n",
            "04/05/2023 04:58:19 - INFO - root - start: 0\n",
            "04/05/2023 04:58:19 - INFO - root - \n",
            "04/05/2023 04:58:19 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: False\n",
            "\n",
            "04/05/2023 04:58:19 - INFO - root - Loading data from SAMSum dataset\n",
            "04/05/2023 04:58:19 - INFO - root - # of Pos: 1576. # of Neg: 9472.\n",
            "loading configuration file ./trained_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"./trained_model/\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 128,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 1,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 1,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
            "loading file ./trained_model/vocab.json\n",
            "loading file ./trained_model/merges.txt\n",
            "loading file ./trained_model/tokenizer.json\n",
            "loading file None\n",
            "loading file ./trained_model/special_tokens_map.json\n",
            "loading file ./trained_model/tokenizer_config.json\n",
            "loading weights file ./trained_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./trained_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on dataset: 100% 2/2 [00:01<00:00,  1.58ba/s]\n",
            "Running tokenizer on dataset: 100% 10/10 [00:05<00:00,  1.83ba/s]\n",
            "04/05/2023 04:58:33 - INFO - __main__ - Sample 853 of the training set: {'input_ids': [0, 19843, 35, 6319, 47, 300, 5, 2225, 31, 5, 558, 116, 50118, 12083, 102, 35, 1491, 648, 6, 38, 437, 164, 89, 3859, 4, 50118, 19843, 35, 370, 1017, 357, 734, 20, 4267, 13, 5, 2502, 16, 273, 23, 5996, 4, 50118, 12083, 102, 35, 5148, 6, 686, 328, 38, 4198, 38, 351, 75, 4309, 24, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 12083, 102, 782, 5, 2225, 31, 5, 558, 7, 3253, 4, 2]}.\n",
            "100% 1576/1576 [01:11<00:00, 21.94it/s]\n",
            "100% 9472/9472 [07:52<00:00, 20.05it/s]\n",
            "04/05/2023 05:07:43 - INFO - root - *** Parameters ***\n",
            "04/05/2023 05:07:43 - INFO - root - load_file: ./data/faceval_samples.json\n",
            "04/05/2023 05:07:43 - INFO - root - neg_marker: ss_neg\n",
            "04/05/2023 05:07:43 - INFO - root - ignore_pad_token_for_loss: True\n",
            "04/05/2023 05:07:43 - INFO - root - max_source_length: 1024\n",
            "04/05/2023 05:07:43 - INFO - root - source_prefix: None\n",
            "04/05/2023 05:07:43 - INFO - root - preprocessing_num_workers: None\n",
            "04/05/2023 05:07:43 - INFO - root - overwrite_cache: True\n",
            "04/05/2023 05:07:43 - INFO - root - min_target_length: 1\n",
            "04/05/2023 05:07:43 - INFO - root - max_target_length: 128\n",
            "04/05/2023 05:07:43 - INFO - root - length_penalty: 1.0\n",
            "04/05/2023 05:07:43 - INFO - root - num_beams: None\n",
            "04/05/2023 05:07:43 - INFO - root - pad_to_max_length: False\n",
            "04/05/2023 05:07:43 - INFO - root - model_name_or_path: ./trained_model/\n",
            "04/05/2023 05:07:43 - INFO - root - config_name: None\n",
            "04/05/2023 05:07:43 - INFO - root - tokenizer_name: None\n",
            "04/05/2023 05:07:43 - INFO - root - text_column: dialogue\n",
            "04/05/2023 05:07:43 - INFO - root - summary_column: summary\n",
            "04/05/2023 05:07:43 - INFO - root - use_slow_tokenizer: False\n",
            "04/05/2023 05:07:43 - INFO - root - per_device_train_batch_size: 8\n",
            "04/05/2023 05:07:43 - INFO - root - per_device_eval_batch_size: 8\n",
            "04/05/2023 05:07:43 - INFO - root - per_device_test_batch_size: 8\n",
            "04/05/2023 05:07:43 - INFO - root - learning_rate: 5e-05\n",
            "04/05/2023 05:07:43 - INFO - root - weight_decay: 0.0\n",
            "04/05/2023 05:07:43 - INFO - root - num_train_epochs: 3\n",
            "04/05/2023 05:07:43 - INFO - root - max_train_steps: None\n",
            "04/05/2023 05:07:43 - INFO - root - gradient_accumulation_steps: 1\n",
            "04/05/2023 05:07:43 - INFO - root - lr_scheduler_type: SchedulerType.LINEAR\n",
            "04/05/2023 05:07:43 - INFO - root - num_warmup_steps: 0\n",
            "04/05/2023 05:07:43 - INFO - root - output_dir: ./scores_log/\n",
            "04/05/2023 05:07:43 - INFO - root - cache_dir: ./output/cache\n",
            "04/05/2023 05:07:43 - INFO - root - seed: 12345\n",
            "04/05/2023 05:07:43 - INFO - root - model_type: bart\n",
            "04/05/2023 05:07:43 - INFO - root - shuffle: False\n",
            "04/05/2023 05:07:43 - INFO - root - label_smoothing: 0.0\n",
            "04/05/2023 05:07:43 - INFO - root - tal: 0.3\n",
            "04/05/2023 05:07:43 - INFO - root - alpha: 1.0\n",
            "04/05/2023 05:07:43 - INFO - root - train_ratio: 1.0\n",
            "04/05/2023 05:07:43 - INFO - root - corrupt_ratio: 0.0\n",
            "04/05/2023 05:07:43 - INFO - root - cf_pair: False\n",
            "04/05/2023 05:07:43 - INFO - root - cf_augmentation: False\n",
            "04/05/2023 05:07:43 - INFO - root - start: 0\n",
            "04/05/2023 05:07:43 - INFO - root - \n",
            "04/05/2023 05:07:43 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: False\n",
            "\n",
            "04/05/2023 05:07:43 - INFO - root - Loading data from SAMSum dataset\n",
            "04/05/2023 05:07:44 - INFO - root - # of Pos: 1576. # of Neg: 2100.\n",
            "loading configuration file ./trained_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"./trained_model/\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 128,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 1,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 1,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
            "loading file ./trained_model/vocab.json\n",
            "loading file ./trained_model/merges.txt\n",
            "loading file ./trained_model/tokenizer.json\n",
            "loading file None\n",
            "loading file ./trained_model/special_tokens_map.json\n",
            "loading file ./trained_model/tokenizer_config.json\n",
            "loading weights file ./trained_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./trained_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  2.86ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:00<00:00,  3.31ba/s]\n",
            "04/05/2023 05:07:53 - INFO - __main__ - Sample 853 of the training set: {'input_ids': [0, 19843, 35, 6319, 47, 300, 5, 2225, 31, 5, 558, 116, 50118, 12083, 102, 35, 1491, 648, 6, 38, 437, 164, 89, 3859, 4, 50118, 19843, 35, 370, 1017, 357, 734, 20, 4267, 13, 5, 2502, 16, 273, 23, 5996, 4, 50118, 12083, 102, 35, 5148, 6, 686, 328, 38, 4198, 38, 351, 75, 4309, 24, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 12083, 102, 782, 5, 2225, 31, 5, 558, 7, 3253, 4, 2]}.\n",
            "100% 1576/1576 [01:17<00:00, 20.44it/s]\n",
            "100% 2100/2100 [01:42<00:00, 20.48it/s]\n",
            "04/05/2023 05:10:59 - INFO - root - *** Parameters ***\n",
            "04/05/2023 05:10:59 - INFO - root - load_file: ./data/faceval_samples.json\n",
            "04/05/2023 05:10:59 - INFO - root - neg_marker: es_neg\n",
            "04/05/2023 05:10:59 - INFO - root - ignore_pad_token_for_loss: True\n",
            "04/05/2023 05:10:59 - INFO - root - max_source_length: 1024\n",
            "04/05/2023 05:10:59 - INFO - root - source_prefix: None\n",
            "04/05/2023 05:10:59 - INFO - root - preprocessing_num_workers: None\n",
            "04/05/2023 05:10:59 - INFO - root - overwrite_cache: True\n",
            "04/05/2023 05:10:59 - INFO - root - min_target_length: 1\n",
            "04/05/2023 05:10:59 - INFO - root - max_target_length: 128\n",
            "04/05/2023 05:10:59 - INFO - root - length_penalty: 1.0\n",
            "04/05/2023 05:10:59 - INFO - root - num_beams: None\n",
            "04/05/2023 05:10:59 - INFO - root - pad_to_max_length: False\n",
            "04/05/2023 05:10:59 - INFO - root - model_name_or_path: ./trained_model/\n",
            "04/05/2023 05:10:59 - INFO - root - config_name: None\n",
            "04/05/2023 05:10:59 - INFO - root - tokenizer_name: None\n",
            "04/05/2023 05:10:59 - INFO - root - text_column: dialogue\n",
            "04/05/2023 05:10:59 - INFO - root - summary_column: summary\n",
            "04/05/2023 05:10:59 - INFO - root - use_slow_tokenizer: False\n",
            "04/05/2023 05:10:59 - INFO - root - per_device_train_batch_size: 8\n",
            "04/05/2023 05:10:59 - INFO - root - per_device_eval_batch_size: 8\n",
            "04/05/2023 05:10:59 - INFO - root - per_device_test_batch_size: 8\n",
            "04/05/2023 05:10:59 - INFO - root - learning_rate: 5e-05\n",
            "04/05/2023 05:10:59 - INFO - root - weight_decay: 0.0\n",
            "04/05/2023 05:10:59 - INFO - root - num_train_epochs: 3\n",
            "04/05/2023 05:10:59 - INFO - root - max_train_steps: None\n",
            "04/05/2023 05:10:59 - INFO - root - gradient_accumulation_steps: 1\n",
            "04/05/2023 05:10:59 - INFO - root - lr_scheduler_type: SchedulerType.LINEAR\n",
            "04/05/2023 05:10:59 - INFO - root - num_warmup_steps: 0\n",
            "04/05/2023 05:10:59 - INFO - root - output_dir: ./scores_log/\n",
            "04/05/2023 05:10:59 - INFO - root - cache_dir: ./output/cache\n",
            "04/05/2023 05:10:59 - INFO - root - seed: 12345\n",
            "04/05/2023 05:10:59 - INFO - root - model_type: bart\n",
            "04/05/2023 05:10:59 - INFO - root - shuffle: False\n",
            "04/05/2023 05:10:59 - INFO - root - label_smoothing: 0.0\n",
            "04/05/2023 05:10:59 - INFO - root - tal: 0.3\n",
            "04/05/2023 05:10:59 - INFO - root - alpha: 1.0\n",
            "04/05/2023 05:10:59 - INFO - root - train_ratio: 1.0\n",
            "04/05/2023 05:10:59 - INFO - root - corrupt_ratio: 0.0\n",
            "04/05/2023 05:10:59 - INFO - root - cf_pair: False\n",
            "04/05/2023 05:10:59 - INFO - root - cf_augmentation: False\n",
            "04/05/2023 05:10:59 - INFO - root - start: 0\n",
            "04/05/2023 05:10:59 - INFO - root - \n",
            "04/05/2023 05:10:59 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: False\n",
            "\n",
            "04/05/2023 05:10:59 - INFO - root - Loading data from SAMSum dataset\n",
            "04/05/2023 05:10:59 - INFO - root - # of Pos: 1576. # of Neg: 643.\n",
            "loading configuration file ./trained_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"./trained_model/\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 128,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 1,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 1,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
            "loading file ./trained_model/vocab.json\n",
            "loading file ./trained_model/merges.txt\n",
            "loading file ./trained_model/tokenizer.json\n",
            "loading file None\n",
            "loading file ./trained_model/special_tokens_map.json\n",
            "loading file ./trained_model/tokenizer_config.json\n",
            "loading weights file ./trained_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./trained_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  2.78ba/s]\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  2.77ba/s]\n",
            "04/05/2023 05:11:06 - INFO - __main__ - Sample 853 of the training set: {'input_ids': [0, 19843, 35, 6319, 47, 300, 5, 2225, 31, 5, 558, 116, 50118, 12083, 102, 35, 1491, 648, 6, 38, 437, 164, 89, 3859, 4, 50118, 19843, 35, 370, 1017, 357, 734, 20, 4267, 13, 5, 2502, 16, 273, 23, 5996, 4, 50118, 12083, 102, 35, 5148, 6, 686, 328, 38, 4198, 38, 351, 75, 4309, 24, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 12083, 102, 782, 5, 2225, 31, 5, 558, 7, 3253, 4, 2]}.\n",
            "100% 1576/1576 [01:15<00:00, 20.90it/s]\n",
            "100% 643/643 [00:36<00:00, 17.72it/s]\n",
            "04/05/2023 05:13:04 - INFO - root - *** Parameters ***\n",
            "04/05/2023 05:13:04 - INFO - root - load_file: ./data/faceval_samples.json\n",
            "04/05/2023 05:13:04 - INFO - root - neg_marker: ps_neg\n",
            "04/05/2023 05:13:04 - INFO - root - ignore_pad_token_for_loss: True\n",
            "04/05/2023 05:13:04 - INFO - root - max_source_length: 1024\n",
            "04/05/2023 05:13:04 - INFO - root - source_prefix: None\n",
            "04/05/2023 05:13:04 - INFO - root - preprocessing_num_workers: None\n",
            "04/05/2023 05:13:04 - INFO - root - overwrite_cache: True\n",
            "04/05/2023 05:13:04 - INFO - root - min_target_length: 1\n",
            "04/05/2023 05:13:04 - INFO - root - max_target_length: 128\n",
            "04/05/2023 05:13:04 - INFO - root - length_penalty: 1.0\n",
            "04/05/2023 05:13:04 - INFO - root - num_beams: None\n",
            "04/05/2023 05:13:04 - INFO - root - pad_to_max_length: False\n",
            "04/05/2023 05:13:04 - INFO - root - model_name_or_path: ./trained_model/\n",
            "04/05/2023 05:13:04 - INFO - root - config_name: None\n",
            "04/05/2023 05:13:04 - INFO - root - tokenizer_name: None\n",
            "04/05/2023 05:13:04 - INFO - root - text_column: dialogue\n",
            "04/05/2023 05:13:04 - INFO - root - summary_column: summary\n",
            "04/05/2023 05:13:04 - INFO - root - use_slow_tokenizer: False\n",
            "04/05/2023 05:13:04 - INFO - root - per_device_train_batch_size: 8\n",
            "04/05/2023 05:13:04 - INFO - root - per_device_eval_batch_size: 8\n",
            "04/05/2023 05:13:04 - INFO - root - per_device_test_batch_size: 8\n",
            "04/05/2023 05:13:04 - INFO - root - learning_rate: 5e-05\n",
            "04/05/2023 05:13:04 - INFO - root - weight_decay: 0.0\n",
            "04/05/2023 05:13:04 - INFO - root - num_train_epochs: 3\n",
            "04/05/2023 05:13:04 - INFO - root - max_train_steps: None\n",
            "04/05/2023 05:13:04 - INFO - root - gradient_accumulation_steps: 1\n",
            "04/05/2023 05:13:04 - INFO - root - lr_scheduler_type: SchedulerType.LINEAR\n",
            "04/05/2023 05:13:04 - INFO - root - num_warmup_steps: 0\n",
            "04/05/2023 05:13:04 - INFO - root - output_dir: ./scores_log/\n",
            "04/05/2023 05:13:04 - INFO - root - cache_dir: ./output/cache\n",
            "04/05/2023 05:13:04 - INFO - root - seed: 12345\n",
            "04/05/2023 05:13:04 - INFO - root - model_type: bart\n",
            "04/05/2023 05:13:04 - INFO - root - shuffle: False\n",
            "04/05/2023 05:13:04 - INFO - root - label_smoothing: 0.0\n",
            "04/05/2023 05:13:04 - INFO - root - tal: 0.3\n",
            "04/05/2023 05:13:04 - INFO - root - alpha: 1.0\n",
            "04/05/2023 05:13:04 - INFO - root - train_ratio: 1.0\n",
            "04/05/2023 05:13:04 - INFO - root - corrupt_ratio: 0.0\n",
            "04/05/2023 05:13:04 - INFO - root - cf_pair: False\n",
            "04/05/2023 05:13:04 - INFO - root - cf_augmentation: False\n",
            "04/05/2023 05:13:04 - INFO - root - start: 0\n",
            "04/05/2023 05:13:04 - INFO - root - \n",
            "04/05/2023 05:13:04 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: False\n",
            "\n",
            "04/05/2023 05:13:04 - INFO - root - Loading data from SAMSum dataset\n",
            "04/05/2023 05:13:04 - INFO - root - # of Pos: 1576. # of Neg: 2990.\n",
            "loading configuration file ./trained_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"./trained_model/\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 128,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 1,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 1,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
            "loading file ./trained_model/vocab.json\n",
            "loading file ./trained_model/merges.txt\n",
            "loading file ./trained_model/tokenizer.json\n",
            "loading file None\n",
            "loading file ./trained_model/special_tokens_map.json\n",
            "loading file ./trained_model/tokenizer_config.json\n",
            "loading weights file ./trained_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./trained_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  2.84ba/s]\n",
            "Running tokenizer on dataset: 100% 3/3 [00:01<00:00,  2.17ba/s]\n",
            "04/05/2023 05:13:14 - INFO - __main__ - Sample 853 of the training set: {'input_ids': [0, 19843, 35, 6319, 47, 300, 5, 2225, 31, 5, 558, 116, 50118, 12083, 102, 35, 1491, 648, 6, 38, 437, 164, 89, 3859, 4, 50118, 19843, 35, 370, 1017, 357, 734, 20, 4267, 13, 5, 2502, 16, 273, 23, 5996, 4, 50118, 12083, 102, 35, 5148, 6, 686, 328, 38, 4198, 38, 351, 75, 4309, 24, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 12083, 102, 782, 5, 2225, 31, 5, 558, 7, 3253, 4, 2]}.\n",
            "100% 1576/1576 [01:14<00:00, 21.12it/s]\n",
            "100% 2990/2990 [02:27<00:00, 20.24it/s]\n",
            "04/05/2023 05:17:02 - INFO - root - *** Parameters ***\n",
            "04/05/2023 05:17:02 - INFO - root - load_file: ./data/faceval_samples.json\n",
            "04/05/2023 05:17:02 - INFO - root - neg_marker: ds_neg\n",
            "04/05/2023 05:17:02 - INFO - root - ignore_pad_token_for_loss: True\n",
            "04/05/2023 05:17:02 - INFO - root - max_source_length: 1024\n",
            "04/05/2023 05:17:02 - INFO - root - source_prefix: None\n",
            "04/05/2023 05:17:02 - INFO - root - preprocessing_num_workers: None\n",
            "04/05/2023 05:17:02 - INFO - root - overwrite_cache: True\n",
            "04/05/2023 05:17:02 - INFO - root - min_target_length: 1\n",
            "04/05/2023 05:17:02 - INFO - root - max_target_length: 128\n",
            "04/05/2023 05:17:02 - INFO - root - length_penalty: 1.0\n",
            "04/05/2023 05:17:02 - INFO - root - num_beams: None\n",
            "04/05/2023 05:17:02 - INFO - root - pad_to_max_length: False\n",
            "04/05/2023 05:17:02 - INFO - root - model_name_or_path: ./trained_model/\n",
            "04/05/2023 05:17:02 - INFO - root - config_name: None\n",
            "04/05/2023 05:17:02 - INFO - root - tokenizer_name: None\n",
            "04/05/2023 05:17:02 - INFO - root - text_column: dialogue\n",
            "04/05/2023 05:17:02 - INFO - root - summary_column: summary\n",
            "04/05/2023 05:17:02 - INFO - root - use_slow_tokenizer: False\n",
            "04/05/2023 05:17:02 - INFO - root - per_device_train_batch_size: 8\n",
            "04/05/2023 05:17:02 - INFO - root - per_device_eval_batch_size: 8\n",
            "04/05/2023 05:17:02 - INFO - root - per_device_test_batch_size: 8\n",
            "04/05/2023 05:17:02 - INFO - root - learning_rate: 5e-05\n",
            "04/05/2023 05:17:02 - INFO - root - weight_decay: 0.0\n",
            "04/05/2023 05:17:02 - INFO - root - num_train_epochs: 3\n",
            "04/05/2023 05:17:02 - INFO - root - max_train_steps: None\n",
            "04/05/2023 05:17:02 - INFO - root - gradient_accumulation_steps: 1\n",
            "04/05/2023 05:17:02 - INFO - root - lr_scheduler_type: SchedulerType.LINEAR\n",
            "04/05/2023 05:17:02 - INFO - root - num_warmup_steps: 0\n",
            "04/05/2023 05:17:02 - INFO - root - output_dir: ./scores_log/\n",
            "04/05/2023 05:17:02 - INFO - root - cache_dir: ./output/cache\n",
            "04/05/2023 05:17:02 - INFO - root - seed: 12345\n",
            "04/05/2023 05:17:02 - INFO - root - model_type: bart\n",
            "04/05/2023 05:17:02 - INFO - root - shuffle: False\n",
            "04/05/2023 05:17:02 - INFO - root - label_smoothing: 0.0\n",
            "04/05/2023 05:17:02 - INFO - root - tal: 0.3\n",
            "04/05/2023 05:17:02 - INFO - root - alpha: 1.0\n",
            "04/05/2023 05:17:02 - INFO - root - train_ratio: 1.0\n",
            "04/05/2023 05:17:02 - INFO - root - corrupt_ratio: 0.0\n",
            "04/05/2023 05:17:02 - INFO - root - cf_pair: False\n",
            "04/05/2023 05:17:02 - INFO - root - cf_augmentation: False\n",
            "04/05/2023 05:17:02 - INFO - root - start: 0\n",
            "04/05/2023 05:17:02 - INFO - root - \n",
            "04/05/2023 05:17:02 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: False\n",
            "\n",
            "04/05/2023 05:17:02 - INFO - root - Loading data from SAMSum dataset\n",
            "04/05/2023 05:17:03 - INFO - root - # of Pos: 1576. # of Neg: 547.\n",
            "loading configuration file ./trained_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"./trained_model/\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 128,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 1,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 1,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
            "loading file ./trained_model/vocab.json\n",
            "loading file ./trained_model/merges.txt\n",
            "loading file ./trained_model/tokenizer.json\n",
            "loading file None\n",
            "loading file ./trained_model/special_tokens_map.json\n",
            "loading file ./trained_model/tokenizer_config.json\n",
            "loading weights file ./trained_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./trained_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on dataset: 100% 2/2 [00:01<00:00,  1.96ba/s]\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  1.91ba/s]\n",
            "04/05/2023 05:17:09 - INFO - __main__ - Sample 853 of the training set: {'input_ids': [0, 19843, 35, 6319, 47, 300, 5, 2225, 31, 5, 558, 116, 50118, 12083, 102, 35, 1491, 648, 6, 38, 437, 164, 89, 3859, 4, 50118, 19843, 35, 370, 1017, 357, 734, 20, 4267, 13, 5, 2502, 16, 273, 23, 5996, 4, 50118, 12083, 102, 35, 5148, 6, 686, 328, 38, 4198, 38, 351, 75, 4309, 24, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 12083, 102, 782, 5, 2225, 31, 5, 558, 7, 3253, 4, 2]}.\n",
            "100% 1576/1576 [01:13<00:00, 21.36it/s]\n",
            "100% 547/547 [00:27<00:00, 19.55it/s]\n",
            "04/05/2023 05:18:59 - INFO - root - *** Parameters ***\n",
            "04/05/2023 05:18:59 - INFO - root - load_file: ./data/faceval_samples.json\n",
            "04/05/2023 05:18:59 - INFO - root - neg_marker: ns_neg\n",
            "04/05/2023 05:18:59 - INFO - root - ignore_pad_token_for_loss: True\n",
            "04/05/2023 05:18:59 - INFO - root - max_source_length: 1024\n",
            "04/05/2023 05:18:59 - INFO - root - source_prefix: None\n",
            "04/05/2023 05:18:59 - INFO - root - preprocessing_num_workers: None\n",
            "04/05/2023 05:18:59 - INFO - root - overwrite_cache: True\n",
            "04/05/2023 05:18:59 - INFO - root - min_target_length: 1\n",
            "04/05/2023 05:18:59 - INFO - root - max_target_length: 128\n",
            "04/05/2023 05:18:59 - INFO - root - length_penalty: 1.0\n",
            "04/05/2023 05:18:59 - INFO - root - num_beams: None\n",
            "04/05/2023 05:18:59 - INFO - root - pad_to_max_length: False\n",
            "04/05/2023 05:18:59 - INFO - root - model_name_or_path: ./trained_model/\n",
            "04/05/2023 05:18:59 - INFO - root - config_name: None\n",
            "04/05/2023 05:18:59 - INFO - root - tokenizer_name: None\n",
            "04/05/2023 05:18:59 - INFO - root - text_column: dialogue\n",
            "04/05/2023 05:18:59 - INFO - root - summary_column: summary\n",
            "04/05/2023 05:18:59 - INFO - root - use_slow_tokenizer: False\n",
            "04/05/2023 05:18:59 - INFO - root - per_device_train_batch_size: 8\n",
            "04/05/2023 05:18:59 - INFO - root - per_device_eval_batch_size: 8\n",
            "04/05/2023 05:18:59 - INFO - root - per_device_test_batch_size: 8\n",
            "04/05/2023 05:18:59 - INFO - root - learning_rate: 5e-05\n",
            "04/05/2023 05:18:59 - INFO - root - weight_decay: 0.0\n",
            "04/05/2023 05:18:59 - INFO - root - num_train_epochs: 3\n",
            "04/05/2023 05:18:59 - INFO - root - max_train_steps: None\n",
            "04/05/2023 05:18:59 - INFO - root - gradient_accumulation_steps: 1\n",
            "04/05/2023 05:18:59 - INFO - root - lr_scheduler_type: SchedulerType.LINEAR\n",
            "04/05/2023 05:18:59 - INFO - root - num_warmup_steps: 0\n",
            "04/05/2023 05:18:59 - INFO - root - output_dir: ./scores_log/\n",
            "04/05/2023 05:18:59 - INFO - root - cache_dir: ./output/cache\n",
            "04/05/2023 05:18:59 - INFO - root - seed: 12345\n",
            "04/05/2023 05:18:59 - INFO - root - model_type: bart\n",
            "04/05/2023 05:18:59 - INFO - root - shuffle: False\n",
            "04/05/2023 05:18:59 - INFO - root - label_smoothing: 0.0\n",
            "04/05/2023 05:18:59 - INFO - root - tal: 0.3\n",
            "04/05/2023 05:18:59 - INFO - root - alpha: 1.0\n",
            "04/05/2023 05:18:59 - INFO - root - train_ratio: 1.0\n",
            "04/05/2023 05:18:59 - INFO - root - corrupt_ratio: 0.0\n",
            "04/05/2023 05:18:59 - INFO - root - cf_pair: False\n",
            "04/05/2023 05:18:59 - INFO - root - cf_augmentation: False\n",
            "04/05/2023 05:18:59 - INFO - root - start: 0\n",
            "04/05/2023 05:18:59 - INFO - root - \n",
            "04/05/2023 05:18:59 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: False\n",
            "\n",
            "04/05/2023 05:18:59 - INFO - root - Loading data from SAMSum dataset\n",
            "04/05/2023 05:18:59 - INFO - root - # of Pos: 1576. # of Neg: 98.\n",
            "loading configuration file ./trained_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"./trained_model/\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 128,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 1,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 1,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
            "loading file ./trained_model/vocab.json\n",
            "loading file ./trained_model/merges.txt\n",
            "loading file ./trained_model/tokenizer.json\n",
            "loading file None\n",
            "loading file ./trained_model/special_tokens_map.json\n",
            "loading file ./trained_model/tokenizer_config.json\n",
            "loading weights file ./trained_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./trained_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on dataset: 100% 2/2 [00:01<00:00,  1.58ba/s]\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 10.65ba/s]\n",
            "04/05/2023 05:19:07 - INFO - __main__ - Sample 853 of the training set: {'input_ids': [0, 19843, 35, 6319, 47, 300, 5, 2225, 31, 5, 558, 116, 50118, 12083, 102, 35, 1491, 648, 6, 38, 437, 164, 89, 3859, 4, 50118, 19843, 35, 370, 1017, 357, 734, 20, 4267, 13, 5, 2502, 16, 273, 23, 5996, 4, 50118, 12083, 102, 35, 5148, 6, 686, 328, 38, 4198, 38, 351, 75, 4309, 24, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 12083, 102, 782, 5, 2225, 31, 5, 558, 7, 3253, 4, 2]}.\n",
            "100% 1576/1576 [01:17<00:00, 20.42it/s]\n",
            "100% 98/98 [00:04<00:00, 22.10it/s]\n",
            "04/05/2023 05:20:35 - INFO - root - *** Parameters ***\n",
            "04/05/2023 05:20:35 - INFO - root - load_file: ./data/faceval_samples.json\n",
            "04/05/2023 05:20:35 - INFO - root - neg_marker: ng_neg\n",
            "04/05/2023 05:20:35 - INFO - root - ignore_pad_token_for_loss: True\n",
            "04/05/2023 05:20:35 - INFO - root - max_source_length: 1024\n",
            "04/05/2023 05:20:35 - INFO - root - source_prefix: None\n",
            "04/05/2023 05:20:35 - INFO - root - preprocessing_num_workers: None\n",
            "04/05/2023 05:20:35 - INFO - root - overwrite_cache: True\n",
            "04/05/2023 05:20:35 - INFO - root - min_target_length: 1\n",
            "04/05/2023 05:20:35 - INFO - root - max_target_length: 128\n",
            "04/05/2023 05:20:35 - INFO - root - length_penalty: 1.0\n",
            "04/05/2023 05:20:35 - INFO - root - num_beams: None\n",
            "04/05/2023 05:20:35 - INFO - root - pad_to_max_length: False\n",
            "04/05/2023 05:20:35 - INFO - root - model_name_or_path: ./trained_model/\n",
            "04/05/2023 05:20:35 - INFO - root - config_name: None\n",
            "04/05/2023 05:20:35 - INFO - root - tokenizer_name: None\n",
            "04/05/2023 05:20:35 - INFO - root - text_column: dialogue\n",
            "04/05/2023 05:20:35 - INFO - root - summary_column: summary\n",
            "04/05/2023 05:20:35 - INFO - root - use_slow_tokenizer: False\n",
            "04/05/2023 05:20:35 - INFO - root - per_device_train_batch_size: 8\n",
            "04/05/2023 05:20:35 - INFO - root - per_device_eval_batch_size: 8\n",
            "04/05/2023 05:20:35 - INFO - root - per_device_test_batch_size: 8\n",
            "04/05/2023 05:20:35 - INFO - root - learning_rate: 5e-05\n",
            "04/05/2023 05:20:35 - INFO - root - weight_decay: 0.0\n",
            "04/05/2023 05:20:35 - INFO - root - num_train_epochs: 3\n",
            "04/05/2023 05:20:35 - INFO - root - max_train_steps: None\n",
            "04/05/2023 05:20:35 - INFO - root - gradient_accumulation_steps: 1\n",
            "04/05/2023 05:20:35 - INFO - root - lr_scheduler_type: SchedulerType.LINEAR\n",
            "04/05/2023 05:20:35 - INFO - root - num_warmup_steps: 0\n",
            "04/05/2023 05:20:35 - INFO - root - output_dir: ./scores_log/\n",
            "04/05/2023 05:20:35 - INFO - root - cache_dir: ./output/cache\n",
            "04/05/2023 05:20:35 - INFO - root - seed: 12345\n",
            "04/05/2023 05:20:35 - INFO - root - model_type: bart\n",
            "04/05/2023 05:20:35 - INFO - root - shuffle: False\n",
            "04/05/2023 05:20:35 - INFO - root - label_smoothing: 0.0\n",
            "04/05/2023 05:20:35 - INFO - root - tal: 0.3\n",
            "04/05/2023 05:20:35 - INFO - root - alpha: 1.0\n",
            "04/05/2023 05:20:35 - INFO - root - train_ratio: 1.0\n",
            "04/05/2023 05:20:35 - INFO - root - corrupt_ratio: 0.0\n",
            "04/05/2023 05:20:35 - INFO - root - cf_pair: False\n",
            "04/05/2023 05:20:35 - INFO - root - cf_augmentation: False\n",
            "04/05/2023 05:20:35 - INFO - root - start: 0\n",
            "04/05/2023 05:20:35 - INFO - root - \n",
            "04/05/2023 05:20:35 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: False\n",
            "\n",
            "04/05/2023 05:20:35 - INFO - root - Loading data from SAMSum dataset\n",
            "04/05/2023 05:20:36 - INFO - root - # of Pos: 1576. # of Neg: 3094.\n",
            "loading configuration file ./trained_model/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"./trained_model/\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 128,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"min_length\": 1,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 1,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
            "loading file ./trained_model/vocab.json\n",
            "loading file ./trained_model/merges.txt\n",
            "loading file ./trained_model/tokenizer.json\n",
            "loading file None\n",
            "loading file ./trained_model/special_tokens_map.json\n",
            "loading file ./trained_model/tokenizer_config.json\n",
            "loading weights file ./trained_model/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./trained_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00,  2.82ba/s]\n",
            "Running tokenizer on dataset: 100% 4/4 [00:01<00:00,  2.77ba/s]\n",
            "04/05/2023 05:20:45 - INFO - __main__ - Sample 853 of the training set: {'input_ids': [0, 19843, 35, 6319, 47, 300, 5, 2225, 31, 5, 558, 116, 50118, 12083, 102, 35, 1491, 648, 6, 38, 437, 164, 89, 3859, 4, 50118, 19843, 35, 370, 1017, 357, 734, 20, 4267, 13, 5, 2502, 16, 273, 23, 5996, 4, 50118, 12083, 102, 35, 5148, 6, 686, 328, 38, 4198, 38, 351, 75, 4309, 24, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 12083, 102, 782, 5, 2225, 31, 5, 558, 7, 3253, 4, 2]}.\n",
            "100% 1576/1576 [01:16<00:00, 20.73it/s]\n",
            "100% 3094/3094 [02:32<00:00, 20.34it/s]\n",
            "= = = = = = = = = = = = = =\n",
            "The project is Finished...\n",
            "The program takes '26' minutes.\n",
            "= = = = = = = = = = = = = =\n"
          ]
        }
      ]
    }
  ]
}